{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# MVP — Predição de Salários (v3, auto-run)\n",
    "\n",
    "\n",
    "\n",
    "**Objetivo**: estimar salário anual a partir de atributos demográficos e profissionais (idade, gênero, escolaridade, cargo e anos de experiência).\n",
    "Tipo de problema: **Regressão supervisionada**\n",
    " já que o alvo (Salary) é uma variável numérica contínua.\n",
    " **1) Carregamento → 2) Preparação → 3) EDA → 4) Treino → 5) Avaliação → 6) Previsão interativa**. Para resolver esse problema, 4 diferentes tipos de regressão foram testados, cada um com suas características:\n",
    "\n",
    "**1**. **Regressão** **Linear**\n",
    "\n",
    "Conceito: É o modelo mais simples de regressão. Assume que existe uma relação linear entre as variáveis de entrada (idade, escolaridade, experiência, etc.) e o salário.\n",
    "\n",
    "Objetivo no projeto: Servir como baseline (referência inicial) para comparar com modelos mais complexos. Apesar de simples, pode capturar relações diretas (ex.: mais anos de experiência → salário mais alto).\n",
    "\n",
    "**2. Random Forest Regressor**\n",
    "\n",
    "Conceito: É um modelo baseado em múltiplas árvores de decisão. Cada árvore aprende de maneira diferente (com amostras variadas e subconjuntos de variáveis) e o resultado final é a média das previsões de todas elas.\n",
    "\n",
    "Objetivo no projeto: Capturar relações não lineares entre as variáveis e o salário, além de reduzir o risco de overfitting, já que o conjunto de árvores é mais estável que uma árvore isolada.\n",
    "\n",
    "**3. Gradient Boosting Regressor**\n",
    "\n",
    "Conceito: Também é baseado em árvores, mas funciona de forma sequencial. Cada nova árvore é treinada para corrigir os erros das árvores anteriores, melhorando a performance gradualmente.\n",
    "\n",
    "Objetivo no projeto: Alcançar maior precisão do que a Random Forest, ao custo de maior tempo de processamento. É especialmente útil quando queremos maximizar a acurácia das previsões.\n",
    "\n",
    "**4. Gradient Boosting Regressor (GridSearch)**\n",
    "A mesma técnica de Gradient Boosting, mas com busca em grade (GridSearchCV) para encontrar melhores hiperparâmetros (número de árvores, taxa de aprendizado, profundidade, etc.).\n",
    "\n",
    "O motivo de testar múltiplos modelos é comparar abordagens diferentes (simples vs complexas) e ver qual tem melhor equilíbrio entre erro baixo e capacidade de generalização.\n",
    "\n",
    "**Hipóteses**:\n",
    "\n",
    "**Mais anos de experiência e maior escolaridade → salários mais altos.**\n",
    "\n",
    "Diferenças por cargo influenciam fortemente o salário.\n",
    "\n",
    "\n",
    "\n",
    "Observação de moeda: pelos valores (ex.: 45k–180k) e faixas de mercado, tratamos salário como USD/ano; se desejar, o caderno pode converter para BRL para contexto local.\n",
    "Restrições/decisões: dados tabulares simples, sem PII, abordagem com modelos clássicos, comparação justa via mesmo pipeline.\n",
    "\n",
    "\n",
    "\n",
    "Atende aos pedidos:\n",
    "- **Carregamento automático do GitHub** (sem upload/sem botões).\n",
    "- **Unificação de Education Level** (*Bachelor's, Master's, PhD*).\n",
    "- **Gráficos com números** (correlação anotada) e **tendência** em *Age × Years_of_Experience*.\n",
    "- **Treino automático** (sem botões) com correção do erro de `squared` → **RMSE = sqrt(MSE)** (compatível com versões antigas do scikit-learn).\n",
    "- **UI de previsão** com `ipywidgets` que **atualiza automaticamente** ao mover os controles (sem clicar).\n",
    "- **(Opcional)** Matriz de confusão **automática** (target discretizado por quantis, apenas didática).\n"
   ],
   "metadata": {
    "id": "dd15f4ec"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "0pgOgXsd4ww7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Configurações & Imports\n"
   ],
   "metadata": {
    "id": "cd60bb68"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Link do GitHub (raw). Altere aqui se necessário.\n",
    "GITHUB_URL = \"https://raw.githubusercontent.com/gabriel-picussa/MVP-de-SSD---Gabriel-Picussa/main/Salary_DatA.xlsx\"\n",
    "\n",
    "# Pacotes\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except Exception:\n",
    "    !pip -q install ipywidgets\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, requests, io, re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "pd.options.mode.copy_on_write = True  # pandas >=2.0\n",
    "\n",
    "def to_raw_github_url(url: str) -> str:\n",
    "    if \"github.com\" in url and \"/blob/\" in url:\n",
    "        parts = url.split(\"github.com/\")[-1]\n",
    "        user_repo, rest = parts.split(\"/blob/\", 1)\n",
    "        return f\"https://raw.githubusercontent.com/{user_repo}/{rest}\"\n",
    "    return url\n"
   ],
   "metadata": {
    "id": "60ef180b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Carregar dados automaticamente do GitHub"
   ],
   "metadata": {
    "id": "bc3ed399"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "RAW = to_raw_github_url(GITHUB_URL)\n",
    "print(\"Lendo:\", RAW)\n",
    "resp = requests.get(RAW, timeout=30)\n",
    "resp.raise_for_status()\n",
    "df = pd.read_excel(io.BytesIO(resp.content))\n",
    "print(\"Dimensões:\", df.shape)\n",
    "display(df.head())\n"
   ],
   "metadata": {
    "id": "769f9dc0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vokgSTsV56ME"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Preparação & Harmonização (auto)\n",
    "Harmonização de escolaridade (foi padronizado as repetições de nomes apenas por erro de escrita,porém são a mesma coisa).\n",
    "\n",
    "“Bachelor’s” ≡ “Bachelor’s Degree” → Bachelor's\n",
    "\n",
    "“Master’s” ≡ “Master’s Degree” → Master's\n",
    "\n",
    "“phD/PhD” → PhD\n",
    "(normalização robusta a caixa/acentos/apóstrofos)."
   ],
   "metadata": {
    "id": "e664f4ab"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def prep_df(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip().replace(' ', '_').replace('-', '_') for c in df.columns]\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    categorical_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "    # Fill básicos\n",
    "    df[numeric_cols] = df[numeric_cols].apply(lambda s: s.fillna(s.median()))\n",
    "    for c in categorical_cols:\n",
    "        df[c] = df[c].astype(str).fillna('Missing').replace({'nan':'Missing'})\n",
    "    # Unificação de Education Level\n",
    "    col = 'Education_Level' if 'Education_Level' in df.columns else ('education_level' if 'education_level' in df.columns else None)\n",
    "    def normalize_edu(v: str) -> str:\n",
    "        t = str(v).strip()\n",
    "        low = t.lower().replace(\"degree\",\"\").replace(\".\",\"\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "        if \"bachelor\" in low: return \"Bachelor's\"\n",
    "        if \"master\"  in low: return \"Master's\"\n",
    "        if \"phd\" in low or \"doutor\" in low: return \"PhD\"\n",
    "        return t\n",
    "    if col: df[col] = df[col].apply(normalize_edu)\n",
    "    return df\n",
    "\n",
    "df = prep_df(df)\n",
    "print(\"Colunas:\", list(df.columns))\n",
    "for c in ['Education_Level','education_level']:\n",
    "    if c in df.columns:\n",
    "        print(c, \"→\", sorted(df[c].unique().tolist()))\n",
    "display(df.head())\n"
   ],
   "metadata": {
    "id": "e4e581ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Seleção automática de Target e Features\n",
    "\n",
    "Esse passo faz a escolha automática do alvo (Target) e das variáveis explicativas (Features) do seu modelo. A função guess_target percorre os nomes das colunas e, por meio de expressões regulares, procura aquela que parece representar salário (salary, remun, pay, wage) e, se encontrar pistas como usd, annual ou year, prioriza essa coluna como TARGET. Em seguida, define FEATURES como todas as demais colunas diferentes do alvo (ex.: idade, gênero, escolaridade, cargo, anos de experiência). **O objetivo é automatizar a preparação do problema de regressão — evitando erros manuais, deixando o notebook plug-and-play e preparando o terreno para a EDA, o train_test_split e o treinamento dos modelos.**"
   ],
   "metadata": {
    "id": "53e7819e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def guess_target(cols):\n",
    "    cands = [c for c in cols if re.search(r\"salary|remun|pay|wage\", str(c), re.I)]\n",
    "    for c in cands:\n",
    "        if re.search(r\"usd|annual|year\", str(c), re.I): return c\n",
    "    return cands[0] if cands else cols[0]\n",
    "\n",
    "TARGET = guess_target(list(df.columns))\n",
    "FEATURES = [c for c in df.columns if c != TARGET]\n",
    "\n",
    "print(\"TARGET =\", TARGET)\n",
    "print(\"FEATURES =\", FEATURES)\n"
   ],
   "metadata": {
    "id": "144e608d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5) EDA automática (com números e tendência)\n",
    "Esse passo não treina modelo ainda — ele faz a Análise Exploratória de Dados (EDA).\n",
    "O objetivo é explorar e visualizar os dados já existentes, para identificar padrões, outliers e relações entre variáveis que podem influenciar no salário.\n",
    "\n",
    "- Os gráficos mostram que a distribuição dos salários é ampla sendo a maioria dos salários (50% do total) fica entre aproximadamente 70 mil e 160 mil, mas com alguns valores bastante altos que aparecem como outliers superiores ou inferiroes.\n",
    "- O boxplot reforça essa dispersão, evidenciando uma mediana próxima de 110 mil e uma cauda longa de salários elevados.\n",
    "- Quando observamos no recorte por escolaridade vemos um crescimento consistente de salário com o nível educacional, onde doutores (PhD) ganham significativamente mais do que mestres, bacharéis e pessoas com apenas ensino médio.\n",
    "- A matriz de correlação e os gráficos de dispersão confirmam relações esperadas: idade e anos de experiência estão fortemente correlacionados entre si (0.94) e ambos se relacionam positivamente com o salário (0.77 e 0.82).\n",
    "- O gráfico de Idade × Experiência mostra uma clara tendência linear, e o de Experiência × Salário demonstra que quanto mais anos de experiência, maior tende a ser a remuneração, apesar da dispersão indicar que outros fatores (como cargo ou setor) também influenciam bastante.\n",
    "- Em conjunto, a análise exploratória valida que variáveis como escolaridade, experiência e idade são bons candidatos para explicar a variação nos salários, justificando o uso da regressão no próximo passo."
   ],
   "metadata": {
    "id": "1de119a6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def find_col_like(patterns, prefer_numeric=True):\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(p in lc for p in patterns):\n",
    "            if not prefer_numeric or pd.api.types.is_numeric_dtype(df[c]):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def trendline(ax, x, y):\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    xx = np.linspace(x.min(), x.max(), 100)\n",
    "    yy = m*xx + b\n",
    "    ax.plot(xx, yy)\n",
    "\n",
    "# Histograma + Boxplot\n",
    "plt.figure(); plt.hist(df[TARGET].dropna(), bins=30); plt.title(f\"Distribuição do alvo: {TARGET}\"); plt.xlabel(TARGET); plt.ylabel(\"Frequência\"); plt.show()\n",
    "plt.figure(); plt.boxplot(df[TARGET].dropna(), labels=[TARGET]); plt.title(f\"Boxplot do alvo: {TARGET}\"); plt.show()\n",
    "\n",
    "# Barras por categorias (até 2 categóricas)\n",
    "cat_feats = [c for c in FEATURES if not pd.api.types.is_numeric_dtype(df[c])]\n",
    "for c in cat_feats[:2]:\n",
    "    top = df[c].value_counts().head(10).index\n",
    "    sub = df[df[c].isin(top)]\n",
    "    means = sub.groupby(c)[TARGET].mean().sort_values(ascending=False)\n",
    "    plt.figure(); means.plot(kind='bar'); plt.title(f\"Média de {TARGET} por {c} (Top 10)\"); plt.xlabel(c); plt.ylabel(f\"Média de {TARGET}\")\n",
    "    plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Correlação com números\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "if len(num_cols) >= 2:\n",
    "    corr = df[num_cols].corr()\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(corr, aspect='auto'); fig.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ax.set_title(\"Matriz de correlação (com valores)\")\n",
    "    ax.set_xticks(range(len(num_cols))); ax.set_yticks(range(len(num_cols)))\n",
    "    ax.set_xticklabels(num_cols, rotation=90); ax.set_yticklabels(num_cols)\n",
    "    for i in range(len(num_cols)):\n",
    "        for j in range(len(num_cols)):\n",
    "            ax.text(j, i, f\"{corr.iloc[i,j]:.2f}\", ha='center', va='center', fontsize=8)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Age × Years_of_Experience com tendência\n",
    "age_col = find_col_like(['age'])\n",
    "exp_col = find_col_like(['experience','years','yoe','exp'])\n",
    "if age_col and exp_col and pd.api.types.is_numeric_dtype(df[age_col]) and pd.api.types.is_numeric_dtype(df[exp_col]):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df[age_col], df[exp_col])\n",
    "    trendline(ax, df[age_col].values, df[exp_col].values)\n",
    "    ax.set_title(f\"{age_col} × {exp_col} (com linha de tendência)\")\n",
    "    ax.set_xlabel(age_col); ax.set_ylabel(exp_col)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# (extra) Experience × TARGET com tendência\n",
    "if exp_col and pd.api.types.is_numeric_dtype(df[exp_col]):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df[exp_col], df[TARGET])\n",
    "    trendline(ax, df[exp_col].values, df[TARGET].values)\n",
    "    ax.set_title(f\"{exp_col} × {TARGET} (com linha de tendência)\")\n",
    "    ax.set_xlabel(exp_col); ax.set_ylabel(TARGET)\n",
    "    plt.tight_layout(); plt.show()\n"
   ],
   "metadata": {
    "id": "c54ca3eb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6) Modelagem automática (Regressão) — comparação e escolha\n",
    "- Os gráficos e métricas mostram que o **Random Forest** foi o modelo mais eficaz, alcançando o maior R² (0,906) e os menores erros (RMSE ≈ 15.489 e MAE ≈ 10.693). Isso significa que ele explica cerca de 91% da variabilidade do salário e apresenta o menor erro médio de previsão em dólares.\n",
    "- O Gradient Boosting com GridSearch também teve bom desempenho (R² = 0,888), mas não superou a Random Forest, enquanto a Regressão Linear ficou bem abaixo (R² = 0,805) e com erros significativamente mais altos, mostrando dificuldade em capturar relações não lineares entre as variáveis e o salário.\n",
    "- De forma geral, os gráficos deixam claro que modelos de árvores são mais adequados para esse tipo de dataset, pois conseguem lidar melhor com interações e padrões complexos do que uma abordagem linear simples. **A escolha do melhor modelo foi feita pelo menor RMSE**, e o Random Forest se destacou como o mais confiável para prever salários neste conjunto de dados."
   ],
   "metadata": {
    "id": "ba2e95e0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "data = df.dropna(subset=[TARGET]).copy()\n",
    "X = data[FEATURES].copy()\n",
    "y = data[TARGET].astype(float)\n",
    "\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "prep = ColumnTransformer([('num', StandardScaler(), num_cols),\n",
    "                          ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)])\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42, n_estimators=300),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([('prep', prep), ('model', model)])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    p = pipe.predict(Xte)\n",
    "    r2 = r2_score(yte, p)\n",
    "    mse = mean_squared_error(yte, p)      # sem 'squared' para compatibilidade\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(yte, p)\n",
    "    results.append((name, pipe, r2, rmse, mae))\n",
    "\n",
    "# GridSearch no GB\n",
    "grid = GridSearchCV(Pipeline([('prep', prep), ('model', GradientBoostingRegressor(random_state=42))]),\n",
    "                    {'model__n_estimators':[100,200], 'model__learning_rate':[0.05,0.1], 'model__max_depth':[2,3]},\n",
    "                    scoring='neg_mean_squared_error', cv=3, n_jobs=-1)  # compatível\n",
    "grid.fit(Xtr, ytr)\n",
    "p = grid.best_estimator_.predict(Xte)\n",
    "r2 = r2_score(yte, p); rmse = float(np.sqrt(mean_squared_error(yte, p))); mae = mean_absolute_error(yte, p)\n",
    "results.append((\"GradientBoosting(GridSearch)\", grid.best_estimator_, r2, rmse, mae))\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[3])\n",
    "print(\"Resultados (ordenado por RMSE):\")\n",
    "for n, _, r2, rmse, mae in results_sorted:\n",
    "    print(f\"{n:30s}  R²={r2:.3f}  RMSE={rmse:.3f}  MAE={mae:.3f}\")\n",
    "\n",
    "labels = [r[0] for r in results_sorted]\n",
    "r2_vals = [r[2] for r in results_sorted]\n",
    "rmse_vals = [r[3] for r in results_sorted]\n",
    "mae_vals = [r[4] for r in results_sorted]\n",
    "\n",
    "plt.figure(); plt.bar(labels, r2_vals); plt.title(\"R²\"); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "plt.figure(); plt.bar(labels, rmse_vals); plt.title(\"RMSE\"); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "plt.figure(); plt.bar(labels, mae_vals); plt.title(\"MAE\"); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "BEST_NAME, BEST_PIPE = results_sorted[0][0], results_sorted[0][1]\n",
    "print(\"Melhor (RMSE):\", BEST_NAME)\n"
   ],
   "metadata": {
    "id": "249ab355"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7) UI de Previsão (sem botão: atualiza ao mover os controles)"
   ],
   "metadata": {
    "id": "aa32b0ef"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def build_controls(features):\n",
    "    ctrls = []\n",
    "    for c in features:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            lo, hi = float(df[c].quantile(0.05)), float(df[c].quantile(0.95))\n",
    "            step = (hi-lo)/100 if hi>lo else 1.0\n",
    "            if pd.api.types.is_float_dtype(df[c]):\n",
    "                w = widgets.FloatSlider(description=c, min=lo, max=hi if hi>lo else lo+1, step=step, value=float(df[c].median()), continuous_update=True)\n",
    "            else:\n",
    "                w = widgets.IntSlider(description=c, min=int(lo), max=int(hi) if hi>0 else int(lo)+1, step=max(1,int(step)), value=int(df[c].median()), continuous_update=True)\n",
    "        else:\n",
    "            opts = sorted(df[c].astype(str).unique().tolist())\n",
    "            w = widgets.Dropdown(description=c, options=opts, value=opts[0] if opts else None)\n",
    "        ctrls.append(w)\n",
    "    return ctrls\n",
    "\n",
    "controls = build_controls(FEATURES)\n",
    "box = widgets.VBox(controls)\n",
    "out = widgets.Output()\n",
    "\n",
    "def update_prediction(change=None):\n",
    "    data = {c.description:[c.value] for c in controls}\n",
    "    Xnew = pd.DataFrame(data)\n",
    "    yhat = BEST_PIPE.predict(Xnew)[0]\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        print(f\"Previsão de salário: {yhat:,.2f}\")\n",
    "\n",
    "for c in controls:\n",
    "    if hasattr(c, 'observe'):\n",
    "        c.observe(update_prediction, names='value')\n",
    "\n",
    "display(box, out)\n",
    "update_prediction()  # faz a primeira previsão automática\n"
   ],
   "metadata": {
    "id": "d40fc54a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8) Classificação derivada do target — matriz de confusão automática\n",
    "Esse passo transforma o problema em classificação só para fins de visualização, e a matriz de confusão mostra que o modelo tem boa capacidade de distinguir entre salários baixos, médios e altos, errando apenas em fronteiras próximas entre as faixas."
   ],
   "metadata": {
    "id": "b6a8377e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Binning do target para fins didáticos (3 faixas por quantis)\n",
    "y_cont = data[TARGET].astype(float).values\n",
    "bins = np.quantile(y_cont, np.linspace(0,1,4))  # 3 bins\n",
    "bins[0] -= 1e-6; bins[-1] += 1e-6\n",
    "y_cat = np.digitize(y_cont, bins[1:-1])\n",
    "Xc = data[FEATURES].copy()\n",
    "\n",
    "num_cols = [c for c in Xc.columns if pd.api.types.is_numeric_dtype(Xc[c])]\n",
    "cat_cols = [c for c in Xc.columns if c not in num_cols]\n",
    "prep_c = ColumnTransformer([('num', StandardScaler(), num_cols), ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)])\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(Xc, y_cat, test_size=0.2, random_state=42)\n",
    "clf = Pipeline([('prep', prep_c), ('model', RandomForestClassifier(random_state=42, n_estimators=300))])\n",
    "clf.fit(Xtr, ytr)\n",
    "yp = clf.predict(Xte)\n",
    "\n",
    "cm = confusion_matrix(yte, yp, labels=[0,1,2])\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_title(\"Matriz de confusão (bins do target)\"); ax.set_xlabel(\"Predito\"); ax.set_ylabel(\"Verdadeiro\")\n",
    "ax.set_xticks([0,1,2]); ax.set_yticks([0,1,2])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, str(cm[i,j]), ha='center', va='center')\n",
    "fig.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "metadata": {
    "id": "33e73fe1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9) Conclusões"
   ],
   "metadata": {
    "id": "670f2521"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Notebook pronto para **rodar tudo de uma vez (Runtime → Run all)**, sem cliques adicionais.\n",
    "- Carrega o dataset do GitHub automaticamente\n",
    "- Harmoniza `Education_Level`\n",
    "- Gera a EDA com números e tendências\n",
    "- Treina e compara os modelos; corrige RMSE para compatibilidade\n",
    "- Mostra uma UI que **atualiza a previsão automaticamente**\n",
    "- Exibe, de forma didática, uma **matriz de confusão** (classificação derivada)\n"
   ],
   "metadata": {
    "id": "d44e0eb1"
   }
  }
 ]
}